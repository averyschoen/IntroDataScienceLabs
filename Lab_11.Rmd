---
title: "Lab 11"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(car)
library(dplyr)
library(GGally)
library(ggplot2)
library(gradethis)
library(learnr)
tutorial_options(exercise.checker = gradethis::grade_learnr)

birthwt <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/MASS/birthwt.csv", 
                    stringsAsFactors = TRUE)
```

## Goals

Your goal is to practice building multiple linear regression models. 

## Part 1: Loading Packages

Don't forget, packages must be loaded any time you want to use them in an R document. For this lab, we will have two new packages--you'll need to install them on your local machine if you haven't already--that will help us when we are building multiple linear regression models, `GGally` for the `ggpairs()` function, and `car` for the `vif()` function.  Calls to load these packages are in the code chunk below. Go ahead and run this code chunk now.

```{r loadpackages, exercise = T}
library(GGally)
library(car)
```

## Part 2: Reading in Data

The following R code reads in the data set a study on factors determining a baby's birthweight and stores it in a dataframe called `birthwt`. Run the code below now to read in the data set.  No need to modify this code, but remember how to use this function to read in data in the future!

```{r birthwt, exercise = T}
birthwt <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/MASS/birthwt.csv", 
                    stringsAsFactors = TRUE)
```

This data frame contains the following columns:

* `low`: indicator of birth weight less than 2.5 kg.
* `age`: mother's age in years.
* `lwt`: mother's weight in pounds at last menstrual period.
* `race`: mother's race (1 = white, 2 = Black, 3 = other).
* `smoke`: smoking status during pregnancy.
* `ptl`: number of previous premature labours.
* `ht`: history of hypertension.
* `ui`: presence of uterine irritability.
* `ftv`: number of physician visits during the first trimester.
* `bwt`: birth weight in grams.

## Part 3: ASN Practice

Similar to Test 1, there will be a section of "Always, Sometimes, Never" questions on the upcoming test. Remember, these are similar to True/False questions, but there are three options rather than two. 

You will be given a statement, which is always, sometimes, or never true. Your job is to select the correct option, depending on the truth of the statement. Tips for answering questions:

* First, consider whether the statement is generally true (Always) or false (Never). You can do this by testing the statement and eliminating one option. Once you choose a general direction, try to find an example *and* a counter example.
* When finding examples and counter examples, make sure to consider any unusual cases! If you are able to find both an example that makes the statement true and an example that makes the statement false, choose "Sometimes". 
* You can also try and clarify *exactly* when the statement is always or never true by crossing out or changing words. This strategy may help you make your decision.

Practice these other Always, Sometimes, Never questions with your group. Talk about how you arrived at each answer. 

```{r ASN1, echo=FALSE}
question("Two variables that have a correlation coefficient near 0 have no relationship.",
  answer("Always", message = "Consider a quadratic relationship!"),
  answer("Sometimes", correct = TRUE),
  answer("Never"),
  allow_retry = T
)
```

```{r ASN2, echo=FALSE}
question("A reduced model has more parameters than a full model.",
  answer("Always"),
  answer("Sometimes"),
  answer("Never", correct = TRUE),
  allow_retry = T
)
```

```{r ASN3, echo=FALSE}
question("In order to include a categorical predictor with $K$ levels in a linear regression model, statistical software creates $K$ indicator variables.",
  answer("Always"),
  answer("Sometimes"),
  answer("Never", correct = TRUE),
  allow_retry = T
)
```

```{r ASN4, echo=FALSE}
question("An $F$-statistic is the square of a $z$-score from a Standard Normal distribution.",
  answer("Always", message = "An $F$-statistic is the square of a $t$-score."),
  answer("Sometimes"),
  answer("Never", correct = TRUE),
  allow_retry = T
)
```

```{r ASN5, echo=FALSE}
question("A confidence interval for $Y_i$ predicted from a value of $X_i$ close to the mean will be narrower than a confidence interval for $Y_j$ predicted from another value of $X_j$ farther from the mean.	",
  answer("Always", correct = TRUE),
  answer("Sometimes"),
  answer("Never"),
  allow_retry = T
)
```

```{r ASN6, echo=FALSE}
question("In a model with at least one predictor, $r^2$ is always smaller than adjusted $r^2$.	",
  answer("Always"),
  answer("Sometimes"),
  answer("Never", correct = TRUE),
  allow_retry = T
)
```

```{r ASN7, echo=FALSE}
question("If two variables $X$ and $Y$ are correlated, $X$ causes $Y$.",
  answer("Always"),
  answer("Sometimes", correct = TRUE),
  answer("Never"),
  allow_retry = T
)
```

```{r ASN8, echo=FALSE}
question("A two-sided test for $H_0: \\beta_1 = 0$ in a simple linear regression model has a p-value less than 0.01. The $F$-test for the overall model also has a significant result.",
  answer("Always", correct = TRUE),
  answer("Sometimes"),
  answer("Never"),
  allow_retry = T
)
```

```{r ASN9, echo=FALSE}
question("If you fail to reject the null hypothesis, you have made a Type II error.",
  answer("Always"),
  answer("Sometimes", correct = TRUE),
  answer("Never"),
  allow_retry = T
)
```

```{r ASN10, echo=FALSE}
question("A prediction interval for $Y_i$ predicted from a specific value of $X_i$ is narrower than a confidence interval for $Y_i$ predicted from the same value of $X_i$. ",
  answer("Always"),
  answer("Sometimes"),
  answer("Never", correct = TRUE),
  allow_retry = T
)
```

## Part 4: Data Exploration

Let's first take a look at the summary statistics for every variable using the `summary()` function. Plug the dataframe into the function.

```{r bwt1, exercise = TRUE}

```

```{r bwt1-solution}
summary(birthwt)
```

```{r bwt1-check, message = FALSE, warning = FALSE}
grade_code()
```

### Storage Types

Notice that `low`, `smoke`, `ht`, `ui`, and `race` are all coded as numeric variables (0 and 1 for `low`, `smoke`, `ht`, and `ui`, and 1, 2, and 3 for `race`), even though in the data documentation, they are more described as categorical variables (e.g., 0 means "No" and 1 means "Yes", or 1 means white, 2 means Black, and 3 means other). It's fine-ish to treat the binary variables as zeroes and ones instead of "Yes"s and "No"s--you'll get the same results either way, but it can be hard to keep track of everything (so just make sure you're aware of which variable is coded as 0 and which is coded as 1). However, a big problem is treating the race variable as a number--this will not give you the results you want! Let's see what happens...

First, fit a model predicting `bwt` from `race` and look at coefficients using the `summary()` function.

```{r bwt2, exercise = TRUE}
model1 <- lm()
summary()
```

```{r bwt2-solution}
model1 <- lm(data = birthwt, bwt ~ race)
summary(model1)
```

```{r bwt2-check, message = FALSE, warning = FALSE}
grade_code()
```

Notice that this model gives a single coefficient for race--essentially, because we are using the values 1, 2, and 3, R thinks that we want a linear regression model. We could theoretically plug in 1 for a white person, 2 for a Black person, and 3 for a person of another race. However, enforcing this numerical difference doesn't really make sense... what does it mean for a Black person (2) to be "one more" than a white person (1), and "one less" than a person of another race (3)? Nothing, really--different races should not have numerical values assigned to them. It's better to change the variable so that it treats race as a factor--this will let R add the appropriate number of categorical variables and treat the model correctly. This can be a bit complicated, but it is a good R skill to have--we've actually already used it once (see Progress Check 6).

Essentially, what we need to do is change the `race` variable so that everywhere the original `race` vector is equal to 1, we want the new `race` vector to say `"white"`, everywhere the original `race` vector is equal to 2, we want the new `race` vector to say `"Black"`, and everywhere the original `race` vector is equal to 3, we want to new `race` version to say `"other"`. We know that this is the relationship from the dataset document--we can adapt the following code:

```{r bwt3, exercise = TRUE}
data <- data %>%
  mutate(variable = factor(variable, labels = c("label1", "label2", "label3")))
```

You want to change the dataframe name, the variable name, and the labels of the dataset (in this case, they would be `"white"`, `"Black"`, and `"other"`). Adapt the code in the chunk below for this exercise. 

```{r bwt4, exercise = TRUE}
data <- data %>%
  mutate(variable = factor(variable, labels = c("label1", "label2", "label3")))
```

```{r bwt4-solution}
birthwt <- birthwt %>%
  mutate(race = factor(race, labels = c("white", "Black", "other")))
```

```{r bwt4-check, message = FALSE, warning = FALSE}
grade_code()
```

Use the `levels()` function to confirm that your code has changed.

```{r prepare-racef1, message = FALSE}
birthwt <- birthwt %>%
  mutate(race = factor(race, labels = c("white", "Black", "other")))
```

```{r bwt5, exercise = TRUE, exercise.setup = "prepare-racef1"}
levels()
```

```{r bwt5-solution}
levels(birthwt$race)
```

```{r bwt5-check, message = FALSE, warning = FALSE}
grade_code()
```

Remember that there's no function that will specifically return the reference level, but if you use `levels()`, the reference will be the first value returned.

```{r bwt6, echo = F}
question(" What is the reference level for `race`?",
  answer("`white`", correct = TRUE),
  answer("`Black`"),
  answer("`other`"),
  allow_retry = TRUE, 
  random_answer_order = TRUE
)
```

Now re-run the `summary()` function to the `race` variable--what have you learned about race now?

```{r bwt7, exercise = TRUE, exercise.setup = "prepare-racef1"}
summary()
```

```{r bwt7-solution}
summary(birthwt$race)
```

```{r bwt7-check, message = FALSE, warning = FALSE}
grade_code()
```

FINALLY, we can fit the new model predicting `bwt` from `race`! Apply the `summary()` function to see what the new model looks like. 

```{r bwt8, exercise = TRUE, exercise.setup = "prepare-racef2"}
model2 <- lm()
summary()
```

```{r bwt8-solution}
model2 <- lm(data = birthwt, bwt ~ race)
summary(model2)
```

```{r bwt8-check, message = FALSE, warning = FALSE}
grade_code()
```

Now, you have two rows--one to compare babies born to Black people to babies born to white people, and one to compare babies born to people of other races to babies born to white people. This is because `"white"` is the reference level! Now, we can see that the conclusions reached with our linear regression model are different. Race still plays a significant role (as it did in `model1`) but this time, we can see that babies born to Black people have significantly different birth weights than babies born to white people, but babies born to people of other races do not have significantly different birthweights than babies born to white people. This is different than before, and illustrates the importance of treating your variables correctly. 

### Exploratory Data Analysis

Now that we have stored the race variable appropriately, we can continue working on our exploratory data analysis. First, find the names of the different variables by applying the `colnames()` function.

```{r bwt9, exercise = TRUE, exercise.setup = "prepare-racef1"}

```

```{r bwt9-solution}
colnames(birthwt)
```

```{r bwt9-check, message = FALSE, warning = FALSE}
grade_code()
```

We are going to apply the `ggpairs()` function to our dataset--note that it does take a minute.

```{r bwt10, exercise = TRUE, exercise.setup = "prepare-racef1"}
ggpairs()
```

```{r bwt10-solution}
ggpairs(birthwt)
```

```{r bwt10-check, message = FALSE, warning = FALSE}
grade_code()
```

There's a lot to unpack here! But one thing that jumps out at me is that there are densities (along the diagonal) for `low`, `smoke`, `ht`, and `ui`, which are categorical variables! That doesn't make sense, but they are coded as 0 and 1 so R thinks they should be summarized with densities rather than bar charts. Let's change that, and re-run the code. 

```{r bwt11, exercise = TRUE, message = FALSE, exercise.setup = "prepare-racef1"}
birthwt <- birthwt %>%
  mutate(low = as.factor(low),
         smoke = as.factor(smoke),
         ht = as.factor(ht), 
         ui = as.factor(ui))

ggpairs(birthwt)
```

First, notice that you can make multiple changes at once! That's useful. Then, we can look at the correlations in the upper half triangle. Are there any pairs of variables that are highly correlated (say, more than 0.7)? These pairs would be considered collinear.

Now, let's look at the multicollinearity of the dataset using the VIF. Do any of the explanatory variables have high VIF (say, more than 10)?

```{r prepare-racef2, message = FALSE}
birthwt <- birthwt %>%
  mutate(race = factor(race, labels = c("white", "Black", "other")), 
         smoke = as.factor(smoke),
         ht = as.factor(ht), 
         ui = as.factor(ui))
```

```{r bwt12, exercise = TRUE, exercise.setup = "prepare-racef2"}
model3 <- lm()
vif()
```

```{r bwt12-solution}
model3 <- lm(data = birthwt, bwt ~ low + age + lwt + race + smoke + ptl + ht + ui + ftv)
vif(model3)
```

```{r bwt12-check, message = FALSE, warning = FALSE}
grade_code()
```

Luckily, most of the variables do not suffer from multicollinearity. However! Take a closer look at the `low` variables and the `birthwt` variables... `low` is defined as an "indicator of birth weight less than 2.5 kg", and `bwt`, our response, is birth weight in grams. Thus, `low` can be found directly from `birthwt`! If we are predicting `birthwt`, it does not make sense to include `low` in our model. You may run into this phenomenon, where one of the predictors is a function of another predictor, in future datasets--if you get an error about "aliasing", this may be what's going on. I recommend looking at the data documentation to be sure.

## Part 5: Model Selection

### Backward Selection

Yesterday, we talked about multiple linear regression and how our ultimate goal is to get a model that fits the data well (high adjusted $r^2$). One procedure that statisticians sometimes use is known as "backward selection". This procedure is roughly as follows:

1. Fit a model with all of the variables. 
2. Identify the variable with the least significant p-value. If the variable is not significant, you can eliminate it from your model. 
3. Repeat steps 1 and 2, eliminating all of the not-significant variables. When you have only significant variables left, you have found your final model!

Let's apply backward selection to this dataset. Start by fitting a full model with the variables `age`, `lwt`, `race`,  `smoke`, `ptl`, `ht`, `ui`, and `ftv`.

```{r bwt13, exercise = TRUE, exercise.setup = "prepare-racef2"}
model4 <- lm()
summary()
```

```{r bwt13-solution}
model4 <- lm(data = birthwt, bwt ~ age + lwt + race + smoke + ptl + ht + ui + ftv)
summary(model4)
```

```{r bwt13-check, message = FALSE, warning = FALSE}
grade_code()
```

Which of the variables would you eliminate, on the basis of their p-values?

Now, fit a new model, eliminating the variable with the least significant p-value.

```{r bwt14, exercise = TRUE, exercise.setup = "prepare-racef2"}
model5 <- lm()
summary()
```

```{r bwt14-solution}
model5 <- lm(data = birthwt, bwt ~ lwt + race + smoke + ptl + ht + ui + ftv)
summary(model5)
```

```{r bwt14-check, message = FALSE, warning = FALSE}
grade_code()
```

Which of the variables would you eliminate, on the basis of their p-values?

Now, fit a new model, eliminating the variable with the least significant p-value.

```{r bwt15, exercise = TRUE, exercise.setup = "prepare-racef2"}
model6 <- lm()
summary()
```

```{r bwt15-solution}
model6 <- lm(data = birthwt, bwt ~ lwt + race + smoke + ptl + ht + ui)
summary(model6)
```

```{r bwt15-check, message = FALSE, warning = FALSE}
grade_code()
```

Which of the variables would you eliminate, on the basis of their p-values?

Now, fit a new model, eliminating the variable with the least significant p-value.

```{r bwt16, exercise = TRUE, exercise.setup = "prepare-racef2"}
model7 <- lm()
summary()
```

```{r bwt16-solution}
model7 <- lm(data = birthwt, bwt ~ lwt + race + smoke + ht + ui)
summary(model7)
```

```{r bwt16-check, message = FALSE, warning = FALSE}
grade_code()
```

Are any of your remaining variables not significant? This is your "final" model--of course, you could try adding interactions, squared terms, or transformations. All may increase the adjusted $R^2$.

### Forward Selection

Another procedure that statisticians sometimes use is known as "forward selection"--I don't use it a lot (and I think you'll see why in a minute), but it is a good option to have in your toolbox. This procedure is roughly as follows:

1. Fit one simple linear regression model predicting your response from each of your variables. 
2. Identify the variable with the most significant p-value. If the variable is significant, you will keep it in your model! 
3. Fit one linear regression model for each of the remaining predictors--instead of being simple linear regression models, they will be multiple linear regression models with the variable from step 2 included every time!
4. Repeat steps 2 and 3 until there are no more significant variables.

Let's apply forward selection to this dataset. Start by fitting a simple linear regression model for each of the variables `age`, `lwt`, `race`,  `smoke`, `ptl`, `ht`, `ui`, and `ftv`.

```{r bwt17, exercise = TRUE, exercise.setup = "prepare-racef2"}
model8 <- lm(data = birthwt, bwt ~ ... )
model9 <- lm(data = birthwt, bwt ~ ... )
model10 <- lm(data = birthwt, bwt ~ ... )
model11 <- lm(data = birthwt, bwt ~ ... )
model12 <- lm(data = birthwt, bwt ~ ... )
model13 <- lm(data = birthwt, bwt ~ ... )
model14 <- lm(data = birthwt, bwt ~ ... )
model15 <- lm(data = birthwt, bwt ~ ... )

summary()
```

```{r bwt17-solution}
model8 <- lm(data = birthwt, bwt ~ age )
model9 <- lm(data = birthwt, bwt ~ lwt)
model10 <- lm(data = birthwt, bwt ~ race)
model11 <- lm(data = birthwt, bwt ~ smoke)
model12 <- lm(data = birthwt, bwt ~ ptl)
model13 <- lm(data = birthwt, bwt ~ ht)
model14 <- lm(data = birthwt, bwt ~ ui)
model15 <- lm(data = birthwt, bwt ~ ftv)

summary(model8)
summary(model9)
summary(model10)
summary(model11)
summary(model12)
summary(model13)
summary(model14)
summary(model15)
```

```{r bwt17-check, message = FALSE, warning = FALSE}
grade_code()
```

You can already see that this has a lot more coding involved, which is why I don't use this procedure often! 

```{r bwt18, echo = F}
question("Which of the variables would you include FIRST, on the basis of their p-values?",
  answer("`ui`", correct = TRUE),
  answer("`age`"),
  answer("`lwt`"),
  answer("`race`"),
  answer("`smoke`"),
  answer("`ptl`"),
  answer("`ht`"),
  answer("`ftv`"),
  allow_retry = TRUE, 
  random_answer_order = TRUE
)
```

Now repeat, but remember to include the same variable you just selected!

```{r bwt19, exercise = TRUE, exercise.setup = "prepare-racef2"}

```

```{r bwt19-solution}
model16 <- lm(data = birthwt, bwt ~ ui + age )
model17 <- lm(data = birthwt, bwt ~ ui + lwt)
model18 <- lm(data = birthwt, bwt ~ ui + race)
model19 <- lm(data = birthwt, bwt ~ ui + smoke)
model20 <- lm(data = birthwt, bwt ~ ui + ptl)
model21 <- lm(data = birthwt, bwt ~ ui + ht)
model22 <- lm(data = birthwt, bwt ~ ui + ftv)

summary(model16)
summary(model17)
summary(model18)
summary(model19)
summary(model20)
summary(model21)
summary(model22)
```

```{r bwt19-check, message = FALSE, warning = FALSE}
grade_code()
```

Now which of the variables would you include, on the basis of their p-values? 

```{r bwt20, echo = F}
question("Which of the variables would you include, on the basis of their p-values?",
  answer("`ht`", correct = TRUE),
  answer("`age`"),
  answer("`lwt`"),
  answer("`race`"),
  answer("`smoke`"),
  answer("`ptl`"),
  answer("`ftv`"),
  allow_retry = TRUE, 
  random_answer_order = TRUE
)
```

Now repeat--keep in mind that the first variable you included, `ui`, should be in all models.

```{r bwt205, exercise = TRUE, exercise.setup = "prepare-racef2"}

```

```{r bwt205-solution}
model23 <- lm(data = birthwt, bwt ~ ui + ht + age )
model24 <- lm(data = birthwt, bwt ~ ui + ht + lwt)
model25 <- lm(data = birthwt, bwt ~ ui + ht + race)
model26 <- lm(data = birthwt, bwt ~ ui + ht + smoke)
model27 <- lm(data = birthwt, bwt ~ ui + ht + ptl)
model28 <- lm(data = birthwt, bwt ~ ui + ht + ftv)

summary(model23)
summary(model24)
summary(model25)
summary(model26)
summary(model27)
summary(model28)
```

```{r bwt205-check, message = FALSE, warning = FALSE}
grade_code()
```

Which of the variables would you include, on the basis of their p-values?

```{r bwt21, echo = F}
question("Which of the variables would you include, on the basis of their p-values?",
  answer("`lwt`", correct = TRUE),
  answer("`age`"),
  answer("`race`"),
  answer("`smoke`"),
  answer("`ptl`"),
  answer("`ftv`"),
  allow_retry = TRUE, 
  random_answer_order = TRUE
)
```

Now repeat--keep in mind that the first two variables you included, `ui` and `ht`, should be in all models.

```{r bwt22, exercise = TRUE, exercise.setup = "prepare-racef2"}

```

```{r bwt22-solution}
model29 <- lm(data = birthwt, bwt ~ ui + ht + lwt + age )
model30 <- lm(data = birthwt, bwt ~ ui + ht + lwt + race)
model31 <- lm(data = birthwt, bwt ~ ui + ht + lwt + smoke)
model32 <- lm(data = birthwt, bwt ~ ui + ht + lwt + ptl)
model33 <- lm(data = birthwt, bwt ~ ui + ht + lwt + ftv)

summary(model29)
summary(model30)
summary(model31)
summary(model32)
summary(model33)
```

```{r bwt22-check, message = FALSE, warning = FALSE}
grade_code()
```

Which of the variables would you include, on the basis of their p-values?

```{r bwt23, echo = F}
question("Which of the variables would you include, on the basis of their p-values?",
  answer("`race`", correct = TRUE),
  answer("`age`"),
  answer("`smoke`"),
  answer("`ptl`"),
  answer("`ftv`"),
  allow_retry = TRUE, 
  random_answer_order = TRUE
)
```

This one is a little tricky! `race` has two values, one for the `"Black"` coefficient and one for the `"other"` coefficient--but since the p-value for `"Black"` is so significant, we will keep both coefficients associated with the `race` variable.

Now repeat--keep in mind that the first three variables you included, `ui`, `ht`, and `lwt`, should be in all models.

```{r bwt24, exercise = TRUE, exercise.setup = "prepare-racef2"}

```

```{r bwt24-solution}
model34 <- lm(data = birthwt, bwt ~ ui + ht + lwt + race + age )
model35 <- lm(data = birthwt, bwt ~ ui + ht + lwt + race + smoke)
model36 <- lm(data = birthwt, bwt ~ ui + ht + lwt + race + ptl)
model37 <- lm(data = birthwt, bwt ~ ui + ht + lwt + race + ftv)

summary(model34)
summary(model35)
summary(model36)
summary(model37)
```

```{r bwt24-check, message = FALSE, warning = FALSE}
grade_code()
```

Which of the variables would you include, on the basis of their p-values? 

```{r bwt25, echo = F}
question("Which of the variables would you include, on the basis of their p-values?",
  answer("`smoke`", correct = TRUE),
  answer("`age`"),
  answer("`ptl`"),
  answer("`ftv`"),
  allow_retry = TRUE, 
  random_answer_order = TRUE
)
```

Now repeat--keep in mind that the first three variables you included, `ui`, `ht`, `lwt`, and `race`, should be in all models.

```{r bwt26, exercise = TRUE, exercise.setup = "prepare-racef2"}

```

```{r bwt26-solution}
model38 <- lm(data = birthwt, bwt ~ ui + ht + lwt + race + smoke + age)
model39 <- lm(data = birthwt, bwt ~ ui + ht + lwt + race + smoke + ptl)
model40 <- lm(data = birthwt, bwt ~ ui + ht + lwt + race + smoke + ftv)

summary(model38)
summary(model39)
summary(model40)
```

```{r bwt26-check, message = FALSE, warning = FALSE}
grade_code()
```

None of the added variables are significant, so I wouldn't keep any of them! Your final model should consist of all of the terms that are significant. Are the terms the same in both models?

Sometimes, they are not! But in this case, and most cases, they are--for that reason, I tend to stick with backward selection since it is much easier to code!

## Part 6: Maximizing Adjusted $r^2$

If you have time remaining, you can move to regular RStudio and try adding additional terms, like squared terms and interactions, to see how high you can get the adjusted $r^2$ to go!

```{r bwt27, exercise = TRUE, exercise.setup = "prepare-racef2"}

```


