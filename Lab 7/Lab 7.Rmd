---
title: "Tutorial"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)
library(learnr)
library(gradethis)
library(reticulate)

library(dplyr)
library(kableExtra)
library(cvAUC)
library(ggrepel)
library(kableExtra)
library(latex2exp)
library(reshape2)


use_virtualenv("~/Desktop/IntroDataScienceLabs/my_env")


custom_checker <- function(label, user_code, solution_code, envir_result, evaluate_result, last_value, stage, ...) {
  if (stage == "code_check") {
      if (user_code==solution_code){
          return(list(message = "Exact match code!", correct = TRUE))
      }
    return(list(message = "Not exact match code", correct = FALSE))
  }
}

gradethis_setup()
tutorial_options(exercise.completion = FALSE, exercise.checker = custom_checker)
```


## Goals

The goals of this lab are:

* To practice basic data cleaning, such as removing columns, dropping rows with missing values, and accessing row names of a dataframe.
* To implement $K$-means clustering, including selecting an appropriate value for $K$ and interpreting the cluster output.
* To implement hierarchical clustering, including creation of dendrograms.

## Setup

For this lab we will be using `plotnine`, `pandas`, `numpy`, `scikit-learn`, `random`, `scipy`, and the dataset `epi_r`. 

```{python setup1, exercise=TRUE, exercise.eval = FALSE, message = FALSE}
import numpy as np
import pandas as pd
import plotnine as p9
import random
import scipy
import sklearn

epi = pd.read_csv('data/epi_r.csv')
```

## Data cleaning and exploration

Recall that the Epicurious dataset, created by a Kaggle User, has over 20,000 recipes from the website Epicurious. Loosely speaking, there are a few groups of variables in the dataset:

* The nutritional variables (`calories`, `protein`, `fat`, `sodium`)
* Ingredient tags (`almond`, `amaretto`, `anchovy`, and so on)
* Place tags (`alabama`, `alaska`, `aspen`, `australia`, and so forth)
* Other tags (`advance.prep.required`, `anthony.bourdain`, etc.)

Before we do anything else, look at the column names of epi.

```{python col, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
epi.columns
```

Notice that the first column is called `title.` Print it out and examine it.

```{python title, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
epi['title']
```

1. We've kind of ignored these type of variables so far--that is, variables that are ID numbers or names. They are technically a type of categorical variable and haven't really been useful to us so far. However, they can be very valuable much later in a clustering analysis, when we try to interpret the clusters. They are the most helpful when they are the row names rather than a separate variable column. Edit the line of code below to "re-import" the data with recipe titles as row names, and remember this trick for later!

```{python imp, exercise = TRUE, message = FALSE, exercise.setup="setup1"}
epi = pd.read_csv('data/epi_r.csv', index_col = ...)
epi.head()
```

```{python imp-solution, message = FALSE, warning = FALSE, echo = FALSE}
epi = pd.read_csv('data/epi_r.csv', index_col = 'title')
epi.head()
```

```{r imp-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

2. Now, check out the dataset with `.describe()`. Do you spot any issues with the data that might slow us down when attempting to analyze it?

```{python setup2, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup1"}
epi = pd.read_csv('data/epi_r.csv', index_col = 'title')
```

```{python desc, exercise = TRUE, message = FALSE, exercise.setup="setup2"}

```

```{python desc-solution, message = FALSE, warning = FALSE, echo = FALSE}
epi.describe()
```

```{r desc-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

Pay careful attention to the `count` row. There should be 20,052 values for each variable, but you should be able to see that at least a few columns (e.g., calories) have fewer. This can be a sign of missing values.\n

In earlier labs, we focused on data cleaning and identifying unusual values in the nutritional variables. We could just drop rows with missing values, for example, with the `.dropna()` method, but I would like to focus on the "tag" variables in this lab, so let's delete the columns with numerical variables entirely.\n

Most of the time, I've been redefining dataframes with only a subset of columns by explicitly naming the variables I would like to keep. However, there are over 600 columns to keep here, and I don't really want to have to name all of them! Instead, I can supply the indices of the columns that I would like to keep.\n

3. The line of code below returns a dataframe with all of the columns. Can you edit it so that I keep only the last 674 columns and drop the numerical variables (a.k.a., the first five columns)? Print out the column names to confirm.

```{python drop, exercise = TRUE, message = FALSE, exercise.setup="setup2"}
epi = epi.iloc[:, :]
epi.columns
```

```{python drop-solution, message = FALSE, warning = FALSE, echo = FALSE}
epi = epi.iloc[:, 5:679]
epi.columns
```

```{r drop-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

## $K$-Means Clustering

Let's take a look at how to cluster with $K$-Means. The `sklearn` function [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) will do this for us--read the linked documentation to brush up on the syntax and see what kind of arguments you can change. I'm particularly interested in:
* n_clusters
* n_init

Obviously, `n_clusters` is the number of clusters. `n_init` is the number of times that we run the clustering algorithm. We mentioned in class that we need to run the clustering algorithm multiple times since there are random starts, this is especially true in large datasets like the one we are working with. \n

4. The following lines of code create 12 clusters using 10 random starts. Can you edit them so that they create 6 clusters with 20 random starts? Save the object as `clust_1` so we can see what outputs are possible in the next few questions.

```{python setup3, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup2"}
epi = epi.iloc[:, 5:679]
```


```{python clust1, exercise = TRUE, message = FALSE, exercise.setup="setup3"}
from sklearn.cluster import KMeans

kmeans1 = KMeans(n_clusters=12, n_init = 10)
clust_1 = kmeans1.fit(epi)
```

```{python clust1-solution, message = FALSE, warning = FALSE, echo = FALSE}
from sklearn.cluster import KMeans

kmeans1 = KMeans(n_clusters=6, n_init = 20)
clust_1 = kmeans1.fit(epi)
```

```{r clust1-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

5. We are interested in a few outputs. First, the within cluster variation--in `sklearn`, this is called something different. Can you identify what object you should be extracting from the documentation? Once you find it, print it in the cell below.

```{python setup4, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup3"}
from sklearn.cluster import KMeans

kmeans1 = KMeans(n_clusters=6, n_init = 20)
clust_1 = kmeans1.fit(epi)
```

```{python iner, exercise = TRUE, message = FALSE, exercise.setup="setup4"}

```

```{python iner-solution, message = FALSE, warning = FALSE, echo = FALSE}
clust_1.inertia_
```

```{r iner-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

6. We used $K=6$ arbitrarily, but of course we need to choose it. As I mentioned in class, we will need to write a loop and create the elbow plot to justify the choice of $K$. Fill in the loop below to get the inertia for values of $K$ from 1 to 15 (I'm picking a large number because the data is so large, in smaller datasets you may not need to go so far).

```{python looped, exercise = TRUE, message = FALSE, exercise.setup="setup4"}
inertias = []

for i in range(1,16):
    kmeans = KMeans(n_clusters=i, n_init = 20)
    kmeans.fit(epi)
    inertias.append(...)
```

```{python looped-solution, message = FALSE, warning = FALSE, echo = FALSE}
inertias = []

for i in range(1,16):
    kmeans = KMeans(n_clusters=i, n_init = 20)
    kmeans.fit(epi)
    inertias.append(kmeans.inertia_)
```

```{r looped-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

7. Now, let's create the elbow plot. First, create a data frame with two columns--one for the value of $K$ and one for the inertia.

```{python setup5, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup4"}
inertias = []

for i in range(1,16):
    kmeans = KMeans(n_clusters=i, n_init = 20)
    kmeans.fit(epi)
    inertias.append(kmeans.inertia_)
```

```{python elbow, exercise = TRUE, message = FALSE, exercise.setup="setup5"}
chooseK = {'K': range(1, 16), 'Inertia': inertias}
chooseK_df = pd.DataFrame(data = ...)
```

```{python elbow-solution, message = FALSE, warning = FALSE, echo = FALSE}
chooseK = {'K': range(1, 16), 'Inertia': inertias}
chooseK_df = pd.DataFrame(data = chooseK)
```

```{r elbow-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

8. Now, run this cell to create a plot with $K$ on the $x$-axis and inertia on the $y$.

```{python setup6, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup5"}
chooseK = {'K': range(1, 16), 'Inertia': inertias}
chooseK_df = pd.DataFrame(data = chooseK)
```

```{python elbowp, exercise=TRUE, message = FALSE, exercise.setup="setup6"}
import plotnine as p9

print(p9.ggplot(chooseK_df, p9.aes(x = 'K', y = 'Inertia')) +
       p9.geom_vline(xintercept = 2, color = "red") + 
       p9.geom_vline(xintercept = 5, color = "red") + 
       p9.geom_vline(xintercept = 6, color = "red") + 
       p9.geom_vline(xintercept = 11, color = "red") +       
       p9.geom_line() +
       p9.scale_x_continuous(name = "$K$") + 
       p9.scale_y_continuous(name = "Inertia") +
       p9.theme(legend_position = "none", figure_size = [6, 3.5]))
```

Note: There does not appear to be a very clear "elbow" in the plot (although it's not terrible, either). This happens sometimes. I might consider the values 2, 5, 6, or 11 based on my plot (yours will look different). Proceed with 11 and create an object `clust_2` on the `epi` dataset.

```{python label, exercise = TRUE, message = FALSE, exercise.setup="setup6"}
kmeans2 = KMeans(n_clusters=..., n_init = 20)
clust_2 = kmeans2.fit(epi)
```

```{python label-solution, message = FALSE, warning = FALSE, echo = FALSE}
kmeans2 = KMeans(n_clusters=11, n_init = 20)
clust_2 = kmeans2.fit(epi)
```

```{r label-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

Now comes the fun part! We can try to interpret each cluster. You might learn more advanced methods for visualizing the clusters in later classes, but in this class, the best we can do is just printing out the "names" of the points in the clusters to try and see if there's a pattern. Adapt the code below to print out the names of the points in Cluster 1, 2, etc. Try and give each one a name--this might depend on context clues, you will really have to think about what they all have in common!

```{python setup7, exercise = FALSE, echo=FALSE, message = FALSE, exercise.setup="setup6"}
kmeans2 = KMeans(n_clusters=11, n_init = 20)
clust_2 = kmeans2.fit(epi)
```

```{python labels, exercise=TRUE, message = FALSE, exercise.setup="setup7"}
epi.index[clust_2.labels_ == 0.0]
```

## Hierarchical Clustering

9. *From Week 7 Course Notes, Monday, May 1* Now let's move on to hierarchical clustering. Remember in class that I mentioned this also goes by the name of agglomerative clustering, which is the name of the `sklearn` command--[`AgglomerativeClustering()`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). Read the linked documentation to brush up on the syntax and see what kind of arguments you can change. I'm particularly interested in:

* `n_clusters`
* `metric`
* `linkage`

`n_clusters` may seem a little odd, because one of the advantages of hierarchical clustering is that we do not have to choose a number of clusters.You can just make this `None` to get all of the dendrogram, but if you wanted to cut your dendrogram, this is one way to do it! That would make it easier to investigate the members of each cluster. `metric` is of course how to measure similarity between two points, and `linkage` is how to measure similarity between groups of points. 

The following line of code creates clusters using `ward` linkage. Can you edit it so that it creates clusters with `complete` linkage? Save the object as `clust_3` so we can see what outputs are possible in the next few questions.

```{python complete, exercise = TRUE, message = FALSE, exercise.setup="setup7"}
from sklearn.cluster import AgglomerativeClustering

clust_3 = AgglomerativeClustering(distance_threshold = 0, n_clusters = None, linkage = 'ward').fit(epi)
```

```{python complete-solution, message = FALSE, warning = FALSE, echo = FALSE}
from sklearn.cluster import AgglomerativeClustering

clust_3 = AgglomerativeClustering(distance_threshold = 0, n_clusters = None, linkage = 'complete').fit(epi)
```

```{r complete-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

9. As mentioned in class, we like the dendrogram output. Unfortunately, `sklearn` does not have a nice function for plotting dendrograms. However, I did find a nice example from `sklearn` for [plotting hierarchical clustering Dendrograms](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py)--it makes use of a function from `scipy`. One other major change is that it uses `matplotlib` syntax rather than `plotnine`, we'll have to wait for `plotnine` to catch up. 

One thing you can do is copy and paste the function `plot_dendrogram` they have written in the tutorial, and apply that function to your own analysis. I've done that for you in the code chunk below--can you plot the dendrogram from the previous question?

```{python dendo, exercise = TRUE, message = FALSE, exercise.setup="setup7"}
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)
    
## PLOT:

plot_dendrogram(..., truncate_mode = "level", p = 12,
                labels = epi.index)
plt.title("Hierarchical Clustering Dendrogram with Complete Linkage")
plt.xlabel("")

plt.show() 
```

```{python dendo-solution, message = FALSE, warning = FALSE, echo = FALSE}
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)
    
## PLOT:

plot_dendrogram(clust_3, truncate_mode = "level", p = 12,
                labels = epi.index)
plt.title("Hierarchical Clustering Dendrogram with Complete Linkage")
plt.xlabel("")

plt.show() 
```

```{r dendo-code-check, message = FALSE, warning = FALSE}
grade_this_code()
```

Unfortunately, this dendrogram isn't very helpful because there are so many observations! 