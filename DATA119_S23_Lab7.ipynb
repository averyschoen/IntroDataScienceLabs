{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6880c48c-16aa-439b-a4be-b327f6c8f2d5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"DATA 119 S23 Lab 7 Solutions\"\n",
    "name: \"Amy Nussbaum\"\n",
    "format: pdf\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371df1e-7ec6-4da4-ae3e-be72208e8211",
   "metadata": {},
   "source": [
    "# Lab 7 Goals\n",
    "\n",
    "The goals of this lab are:\n",
    "\n",
    "* To practice basic data cleaning, such as removing columns, dropping rows with missing values, and accessing row names of a dataframe.\n",
    "* To implement $K$-means clustering, including selecting an appropriate value for $K$ and interpreting the cluster output. \n",
    "* To implement hierarchical clustering, including creation of dendrograms.\n",
    "\n",
    "For this lab, it may be helpful to install and load the following modules:\n",
    " \n",
    "* `matplotlib` \n",
    "* `numpy`\n",
    "* `pandas`\n",
    "* `plotnine`\n",
    "* `random`\n",
    "* `scipy`\n",
    "* `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc16586-f9b8-45ec-8564-14b216d1923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "import random\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d689bf-8e7b-47a9-b554-74be4f57d564",
   "metadata": {},
   "source": [
    "We will be using the Epicurious dataset from Labs 1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988c2d6-31e0-4916-810c-f4c48a64d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = pd.read_csv('/Users/amynussbaum/Documents/U of C/Courses/119/Week 1/Lab/epi_r.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09b924-1c4e-46b1-b6a4-8b2a5e6d0167",
   "metadata": {},
   "source": [
    "# Data Cleaning and Exploration\n",
    "\n",
    "Recall that the Epicurious dataset, created by a Kaggle User, has [over 20,000 recipes from the website Epicurious](https://www.kaggle.com/datasets/hugodarwood/epirecipes/code). Loosely speaking, there are a few groups of variables in the dataset:\n",
    "  \n",
    "* The nutritional variables (`calories`, `protein`, `fat`, `sodium`)\n",
    "* Ingredient tags (`almond`, `amaretto`, `anchovy`, and so on)\n",
    "* Place tags (`alabama`, `alaska`, `aspen`, `australia`, and so forth)\n",
    "* Other tags (`advance.prep.required`, `anthony.bourdain`, etc.)\n",
    "\n",
    "1. Before we do anything else, look at the column names of `epi`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b9817-c1a5-4c42-9f3d-204435787594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae1896e6-ec3a-416c-b253-af51362b8a38",
   "metadata": {},
   "source": [
    "2. Notice that the first column is called `title`. Print it out and examine it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836abf4b-67f5-4815-82d0-d5424514ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "703bcb93-433b-4fee-a74c-50d22a07f70f",
   "metadata": {},
   "source": [
    "3. We've kind of ignored these type of variables so far--that is, variables that are ID numbers or names. They are technically a type of categorical variable and haven't really been useful to us so far. However, they can be very valuable much later in a clustering analysis, when we try to interpret the clusters. They are the most helpful when they are the row names rather than a separate variable column. Edit the line of code below to \"re-import\" the data with recipe titles as row names, and remember this trick for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73990bcd-9fc5-4674-aa04-c7b41b2cbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = pd.read_csv('/Users/amynussbaum/Documents/U of C/Courses/119/Week 1/Lab/epi_r.csv', index_col = 'title')\n",
    "epi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0544366a-df51-4c49-81c5-d03627935279",
   "metadata": {},
   "source": [
    "4. Now, check out the dataset with the usual suspects (`.head()`, `.shape()`, `.describe()`, etc.). Do you spot any issues with the data that might slow us down when attempting to analyze it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889618fb-ee70-4ef5-83ba-6c85fd250591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51428179-63fc-43c7-b885-72a969785d20",
   "metadata": {},
   "source": [
    "5. Pay careful attention to the `count` row. There should be 20,052 values for each variable, but you should be able to see that at least a few columns (e.g., calories) have fewer. This can be a sign of missing values.\n",
    "\n",
    "If a column has less values than that given in the output of `epi.shape`, some of the values in that column are missing. Examine the following line of code to see what it is doing, and then run it. How many columns having at least one missing value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d861af6-3cf2-4e20-89fa-8890b8a4d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(epi.describe().iloc[0,] < 20052)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f9509-3b99-4b72-a897-cc692eda7ac8",
   "metadata": {},
   "source": [
    "7. In earlier labs, we focused on data cleaning and identifying unusual values in the nutritional variables. We could just drop rows with missing values, for example, with the `.dropna()` method, but I would like to focus on the \"tag\" variables in this lab, so let's delete the columns with numerical variables entirely.\n",
    "\n",
    "Most of the time, I've been redefining dataframes with only a subset of columns by explicitly naming the variables I would like to keep. However, there are over 600 columns to keep here, and I don't really want to have to name all of them! Instead, I can supply the indices of the columns that I would like to keep. \n",
    "\n",
    "The line of code below returns a dataframe with all of the columns. Can you edit it so that I keep only the last 674 columns and drop the numerical variables (a.k.a., the first five columns)? Print out the column names to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b65953-85dd-412a-98ab-7576a78144b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epi.iloc[:, 0:679]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e5770-11c9-451f-8380-6b1523a5f4c3",
   "metadata": {},
   "source": [
    "# $K$-Means Clustering\n",
    "\n",
    "8. *From Week 7 Course Notes, Monday, May 1* Let's take a look at how to cluster with $K$-Means. The `sklearn` function [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) will do this for us--read the linked documentation to brush up on the syntax and see what kind of arguments you can change. I'm particularly interested in:\n",
    "\n",
    "* `n_clusters`\n",
    "* `n_init`\n",
    "\n",
    "Obviously, `n_clusters` is the number of clusters. `n_init` is the number of times that we run the clustering algorithm. We mentioned in class that we need to run the clustering algorithm multiple times since there are random starts, this is especially true in large datasets like the one we are working with. \n",
    "\n",
    "The following lines of code create 12 clusters using 10 random starts. Can you edit them so that they create 6 clusters with 20 random starts? Save the object as `clust_1` so we can see what outputs are possible in the next few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08324b66-89ad-4938-a4e2-7758590bd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(944) ## keep this for reproducibility! You can change the seed if you want.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans1 = KMeans(n_clusters=12, n_init = 10)\n",
    "clust_1 = kmeans1.fit(epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f23ef5-9a19-4387-b974-f88546d49966",
   "metadata": {},
   "source": [
    "9. We are interested in a few outputs. First, the within cluster variation--in `sklearn`, this is called something different. Can you identify what object you should be extracting from the documentation? Once you find it, print it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b1bce-fc52-4f37-a20c-5cd1ccae47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc7e42-f5eb-4c8a-ab9e-38712ecc119a",
   "metadata": {},
   "source": [
    "10. We used $K=6$ arbitrarily in Step 8., but of course we need to choose it. As I mentioned in class, we will need to write a loop and create the elbow plot to justify the choice of $K$. Fill in the loop below to get the inertia for values of $K$ from 1 to 25 (I'm picking a large number because the data is so large, in smaller datasets you may not need to go so far)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e9320-c2d0-49ed-b75b-3370069b9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "\n",
    "for i in range():\n",
    "    kmeans = KMeans()\n",
    "    kmeans.fit()\n",
    "    inertias.append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f21b1b-23ff-4137-9bbb-5a57f0bfa015",
   "metadata": {},
   "source": [
    "11. Now, let's create the elbow plot. First, create a data frame with two columns--one for the value of $K$ and one for the inertias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce334158-7adc-4716-a985-b4db29eae619",
   "metadata": {},
   "outputs": [],
   "source": [
    "chooseK = {}\n",
    "chooseK_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3324f-8455-4f18-b45e-9bb3b718f7df",
   "metadata": {},
   "source": [
    "12. Now, create the plot with $K$ on the $x$-axis and inertia on the $y$. Do you see a clear choice for $K$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e3533-749b-419d-a62f-403fcf008a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "\n",
    "print(p9.ggplot() +\n",
    "       p9.geom_line())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efd007-e572-41f9-9357-83c8933ab36e",
   "metadata": {},
   "source": [
    "13. There does not appear to be a very clear \"elbow\" in the plot (although it's not terrible, either). This happens sometimes. I might consider the values 2, 5, 6, or 11 based on my plot (yours will look different). Pick a value and create an object `clust_2` on the `epi` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962efc0-8c83-4b52-b523-2d9f06459a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2 = KMeans()\n",
    "clust_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030d4e8-ccd0-4b1d-8c19-ff23461077ca",
   "metadata": {},
   "source": [
    "14. One other thing we are interested in are the actual labels. Using your `clust_2` object and the `sklearn` documentation, can you figure out how to print them out?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf78f0a-0143-490a-80b9-a7a4ca4747bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5a322-c332-4804-ba9e-fb93005485c0",
   "metadata": {},
   "source": [
    "15. Now comes the fun part! We can try to interpret each cluster. You might learn more advanced methods for visualizing the clusters in later classes, but in this class, the best we can do is just printing out the \"names\" of the points in the clusters to try and see if there's a pattern. Adapt the code below to print out the names of the points in Cluster 1, 2, etc. Try and give each one a name--this might depend on context clues, you will really have to think about what they all have in common!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2728c-c875-40d3-8198-0a4e6ab5420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.index[clust_2.labels_ == 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc45ae2-b8c2-478c-acaf-62e69980a8c7",
   "metadata": {},
   "source": [
    "16. If you have time, try playing with some of the other settings in `KMeans()`. Do your clusters appear relatively stable?\n",
    "\n",
    "# Hierarchical Clustering \n",
    "\n",
    "17. *From Week 7 Course Notes, Monday, May 1* Now let's move on to hierarchical clustering. Remember in class that I mentioned this also goes by the name of agglomerative clustering, which is the name of the `sklearn` command--[`AgglomerativeClustering()`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). Read the linked documentation to brush up on the syntax and see what kind of arguments you can change. I'm particularly interested in:\n",
    "\n",
    "* `n_clusters`\n",
    "* `metric`\n",
    "* `linkage`\n",
    "\n",
    "`n_clusters` may seem a little odd, because one of the advantages of hierarchical clustering is that we do not have to choose a number of clusters.You can just make this `None` to get all of the dendrogram, but if you wanted to cut your dendrogram, this is one way to do it! That would make it easier to investigate the members of each cluster. `metric` is of course how to measure similarity between two points, and `linkage` is how to measure similarity between groups of points. \n",
    "\n",
    "The following line of code creates clusters using `ward` linkage. Can you edit it so that it creates clusters with `complete` linkage? Save the object as `clust_3` so we can see what outputs are possible in the next few questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e99a9-4676-4a1b-867b-d50198fb1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clust_3 = AgglomerativeClustering(distance_threshold = 0, n_clusters = None, \n",
    "                                  linkage = 'ward').fit(epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c33b40-82d6-4d65-a67e-4940fd445278",
   "metadata": {},
   "source": [
    "18. As mentioned in class, we like the dendrogram output. Unfortunately, `sklearn` does not have a nice function for plotting dendrograms. However, I did find a nice example from `sklearn` for [plotting hierarchical clustering Dendrograms](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py)--it makes use of a function from `scipy`. One other major change is that it uses `matplotlib` syntax rather than `plotnine`, we'll have to wait for `plotnine` to catch up. \n",
    "\n",
    "One thing you can do is copy and paste the function `plot_dendrogram` they have written in the tutorial, and apply that function to your own analysis. I've done that for you in the code chunk below--can you plot the dendrogram from the previous question?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39add3e9-0234-4d52-bf50-b46931906b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904eca2c-a6c1-49bd-b4e0-9f5b7e7c76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()                                      \n",
    "plt.title(\"\")\n",
    "plt.xlabel(\"\")\n",
    "\n",
    "plot_dendrogram()\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58ac52-6091-4fab-b8b5-7472c4f0f294",
   "metadata": {},
   "source": [
    "19. Unfortunately, this dendrogram isn't very helpful because there are so many observations! Let's try re-running your clusters with the same value of $K$ you used previously. Are your clusters somewhat stable, or did the interpretations change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16fc91-2f91-44d8-8029-224aae3eb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_3 =\n",
    "\n",
    "epi.index[clust_3.labels_ == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b6e54-6241-49b2-b8fe-fe82c14cb059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
