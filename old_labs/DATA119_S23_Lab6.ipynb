{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6880c48c-16aa-439b-a4be-b327f6c8f2d5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"DATA 119 S23 Lab 6 Solutions\"\n",
    "name: \"Amy Nussbaum\"\n",
    "format: pdf\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371df1e-7ec6-4da4-ae3e-be72208e8211",
   "metadata": {},
   "source": [
    "# Lab 6 Goals\n",
    "\n",
    "The goals of this lab are:\n",
    "\n",
    "* To implement $k$NN classification and regression. \n",
    "* To create a graph illustrating the effects of choosing $k$ on training and test set accuracy. \n",
    "* To implement neural networks using `keras` and `tensorflow`.\n",
    "\n",
    "For this lab, it may be helpful to install and load the following modules:\n",
    " \n",
    "* `numpy`\n",
    "* `pandas`\n",
    "* `plotnine`\n",
    "* `random`\n",
    "* `sklearn`\n",
    "* `tensorflow`\n",
    "\n",
    "`tensorflow` can be particularly tricky to install! If you are unsuccessful, I recommend trying the steps in this [tutorial for installing `tensorflow` with Anaconda](https://docs.anaconda.com/free/anaconda/applications/tensorflow/). You may have to download Miniconda, but you should be able to install the code in only a few steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc16586-f9b8-45ec-8564-14b216d1923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "import random\n",
    "import sklearn\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d689bf-8e7b-47a9-b554-74be4f57d564",
   "metadata": {},
   "source": [
    "We will be using the birthweight dataset from Homeworks 3 and 4. Refresh yourself on the variables it includes by reading the [documentation](https://vincentarelbundock.github.io/Rdatasets/doc/MASS/birthwt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988c2d6-31e0-4916-810c-f4c48a64d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthwt = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09b924-1c4e-46b1-b6a4-8b2a5e6d0167",
   "metadata": {},
   "source": [
    "0. As a preliminary step, go ahead and create indicator variables for the categorical variables. Then, \"scramble\" the rows so that when we are doing cross validation, we're not accidentally fitting models on unrepresentative data. If you want EXACTLY the same results as I have, use `random.seed(137)` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b9817-c1a5-4c42-9f3d-204435787594",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(137)\n",
    "\n",
    "birthwt = \n",
    "birthwt = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1896e6-ec3a-416c-b253-af51362b8a38",
   "metadata": {},
   "source": [
    "# $k$NN\n",
    "\n",
    "1. Recall that in Homeworks 3 and 4 we were interested in predicting `low` from `age`, `lwt`, `smoke`, `ptl`, `ht`, `ui`, `ftv`, and `race` (but not `bwt`, since that will predict `low` perfectly). First, read the documentation for [$k$NN Classification](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and [$k$NN Regression](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html). For predicting `low`, which is the most appropriate method to use? Split the data into `X` and `Y` dataframes to use later, and go ahead and scale the `X` data, storing it as `X_pp`, for use in our $k$NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836abf4b-67f5-4815-82d0-d5424514ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \n",
    "Y = \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit()\n",
    "\n",
    "X_pp = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bcb93-433b-4fee-a74c-50d22a07f70f",
   "metadata": {},
   "source": [
    "2. *From Week 6 Course Notes, Monday, April 24:* Using the correct command from Step 1., write a line of code that specifies the number of neighbors in your classifier or regressor as $k = 5$. No need to fit anything yet, but name the model `knn5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73990bcd-9fc5-4674-aa04-c7b41b2cbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5 = sklearn.neighbors.KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0544366a-df51-4c49-81c5-d03627935279",
   "metadata": {},
   "source": [
    "3. Now, fit your classifier or regressor from Step 2. with `X_pp` and `Y` from Step 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889618fb-ee70-4ef5-83ba-6c85fd250591",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5_fit = knn5.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51428179-63fc-43c7-b885-72a969785d20",
   "metadata": {},
   "source": [
    "4. Next, apply `.predict()` to get the predictions from the model in Step 3 (name them `knn5_preds` to use for later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d861af6-3cf2-4e20-89fa-8890b8a4d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5_preds = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f9509-3b99-4b72-a897-cc692eda7ac8",
   "metadata": {},
   "source": [
    "5. *Week 4 Course Notes, Wednesday, April 12* Now, use `pd.crosstabs()` to calculate a confusion matrix for this model. What are the accuracy, sensitivity, and specificity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b65953-85dd-412a-98ab-7576a78144b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e5770-11c9-451f-8380-6b1523a5f4c3",
   "metadata": {},
   "source": [
    "6. Note that when you calculated the predictions, you were returned an array of 0's and 1's corresponding to a non-low birthweight and a low birthweight, respectively. This is not what we are used to--with the results of a logistic regression, for example, we see probabilities. To get probabilities out of a $k$NN model, use `.predict_proba()` and save the new predictions as `knn5_preds_proba`. What kind of results do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08324b66-89ad-4938-a4e2-7758590bd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5_preds_proba = knn5_fit.predict_proba(X_pp)\n",
    "knn5_preds_proba.shape\n",
    "knn5_preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f23ef5-9a19-4387-b974-f88546d49966",
   "metadata": {},
   "source": [
    "7. *Week 5 Course Notes, Monday, April 17* Extract the second column of `knn5_preds_proba` to use as the predicted probabilities. Use that column, along with the column for the response variable, to calculate a ROC curve and value for AUC. Is this model a good model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b1bce-fc52-4f37-a20c-5cd1ccae47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, accuracy_score)\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(,\n",
    "                                           pos_label=1)\n",
    "print(metrics.auc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc7e42-f5eb-4c8a-ab9e-38712ecc119a",
   "metadata": {},
   "source": [
    "# Choosing $k$\n",
    "\n",
    "8. *From Week 6 Course Notes, Monday, April 24:* Let's move on to choosing the best value of $k$ for this data. Remember that we typically do so by using cross-validation, or at least using some model selection statistic on a test set. Let's proceed with taking a 70/30 split of our data. We have already randomized the rows, so we can actually just take the first 70\\% of our data for training and keep the last 30\\% for testing. Go ahead and do that now, but don't forget that you will need `X_in`, `X_out`, `Y_in`, and `Y_out`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e9320-c2d0-49ed-b75b-3370069b9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = X.iloc[]\n",
    "X_out = X.iloc[]\n",
    "\n",
    "Y_in = Y.iloc[]\n",
    "Y_out = Y.iloc[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f21b1b-23ff-4137-9bbb-5a57f0bfa015",
   "metadata": {},
   "source": [
    "9. Remember that we need to scale the data as well--if we used the scaled data from before, it would be scaled using statistics from all of the data, not just the training data. Go ahead and rescale `X_in` and `X_out` so that they both have column means all equal to 0 and column standard deviations all equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce334158-7adc-4716-a985-b4db29eae619",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit()\n",
    "\n",
    "X_in_pp = pd.DataFrame(scaler.transform(), columns = [['age', 'lwt', 'smoke', \n",
    "                                                           'ptl', 'ht', 'ui', 'ftv', \n",
    "                                                           'race_2', 'race_3']])\n",
    "                                                          \n",
    "scaler = StandardScaler()\n",
    "scaler.fit()\n",
    "\n",
    "X_out_pp = pd.DataFrame(scaler.transform(), columns = [['age', 'lwt', 'smoke', \n",
    "                                                             'ptl', 'ht', 'ui', 'ftv', \n",
    "                                                             'race_2', 'race_3']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3324f-8455-4f18-b45e-9bb3b718f7df",
   "metadata": {},
   "source": [
    "10. *Week 5 Course Notes, Monday, April 17* Now, we need to write a loop again. The loop needs to search over a range of $k$ (I used 1 to 20), and in each iteration, fit a $k$NN model using the training set with the new value of $k$, predict the classes of the test set, and summarize the performance of the model using some appropriate value. Think back to your response to Step 1.--what are some appropriate model selection statistics that you can use for the type of response you chose?\n",
    "\n",
    "11. For simplicity, let's use accuracy. Fill in the loop below, storing the results from the test set in `metric`. You may want to use your code from Steps 3, 4, and 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e3533-749b-419d-a62f-403fcf008a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = []\n",
    "\n",
    "for k in range(1, 21):\n",
    "  tempknn = sklearn.neighbors.KNeighborsRegressor()\n",
    "  tempknn_preds = tempknn.fit().predict() > 0.5 \n",
    "  tempknn_cm = pd.crosstab()\n",
    "  metric.append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efd007-e572-41f9-9357-83c8933ab36e",
   "metadata": {},
   "source": [
    "12. Which value of $k$ results in the highest test set accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962efc0-8c83-4b52-b523-2d9f06459a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e030d4e8-ccd0-4b1d-8c19-ff23461077ca",
   "metadata": {},
   "source": [
    "13. Now, run the code below to create a plot showing the relationship between $k$ and test set accuracy. How would you describe this relationship? See if you can add a vertical line representing the best value of $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf78f0a-0143-490a-80b9-a7a4ca4747bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_kNN = {'k': range(1, 21), 'Metric': metric}\n",
    "kNN_plot = pd.DataFrame(data = d_kNN)  \n",
    "\n",
    "print(p9.ggplot(kNN_plot, p9.aes(x = 'k', y = 'Metric')) +\n",
    "       p9.geom_line() +\n",
    "       p9.geom_vline(xintercept = ) + \n",
    "       p9.scale_x_continuous(name = \"$k$\") + \n",
    "       p9.scale_y_continuous(name = \"Test Set Accuracy\") +\n",
    "       p9.theme(legend_position = \"none\", figure_size = [6, 3.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5a322-c332-4804-ba9e-fb93005485c0",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "*From Week 6 Course Notes, Wednesday, April 26* As you know, neural networks are a popular type of deep learning algorithms. They are made up of networks of functions and can either perform classification or regression tasks. Recall that they have the following components:\n",
    "\n",
    "* Input Layer\n",
    "* Hidden Layer \n",
    "* Activation Functions\n",
    "* Weights\n",
    "* Output Layer\n",
    "\n",
    "When coding a neural network, you need to take the time to properly set up each layer, but compiling the models is reasonably straightforward.\n",
    "\n",
    "14. Let's begin by first creating a `Sequential` model--call it `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2728c-c875-40d3-8198-0a4e6ab5420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc45ae2-b8c2-478c-acaf-62e69980a8c7",
   "metadata": {},
   "source": [
    "15. This code creates a structure to which we can add layers. Remember that each layer consists of a certain amount of units--the first layer takes in `input_dim` units, where `input_dim` is the number of features from the input layer. How many units should there be in the first layer if we are using the training and test sets you created from the `birthwt` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e99a9-4676-4a1b-867b-d50198fb1b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37c33b40-82d6-4d65-a67e-4940fd445278",
   "metadata": {},
   "source": [
    "16. When we actually add the layer, we are going to use the `.add()` method with the `Dense()` function (both from `tensorflow`--it turns out that these methods are fairly good at detecting the number of input units, so you don't need to actually add `input_dim` to your function, but let's do it here anyway). You also have to add the number of units that come out of the layer with `units`, as well as an `activation` function. Explore the [documentation for `Dense()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)--what options do you see for `activation`?\n",
    "\n",
    "17. Now, using `.add()` and `Dense()`, add a layer with 4 units that uses the ReLu activation function to your model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39add3e9-0234-4d52-bf50-b46931906b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(units = , input_dim = , activation = ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58ac52-6091-4fab-b8b5-7472c4f0f294",
   "metadata": {},
   "source": [
    "18. We could continue adding hidden layers (and I encourage you to try that eventually, perhaps on HW5), but let's review how to add the output layer. You use the same `.add()` and `Dense()` functions, but we have to pay careful attention to the number of units--we only need one unit since we just want one number (per observation). \n",
    "\n",
    "We also need to pay close attention to the activation function for the output layer. For a regression task, we need a special function, the identity function. This function takes in $x$ and returns $x$ itself. We didn't cover it in class this week, but I did mention it a while ago--this function is the `linear` function in `tensorflow`. \n",
    "\n",
    "Again, for a classification task, you need something different. However, you've already seen the possible options-- `'relu'` should work, as should `'softmax'` (which we did not cover in class). \n",
    "\n",
    "Add the output layer to your mode using `activation = 'softmax'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16fc91-2f91-44d8-8029-224aae3eb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa7474a-6465-45ba-8115-7b91fccfeaaf",
   "metadata": {},
   "source": [
    "19. Now, it is time to compile your model with `.compile()`. Recall that training a network means finding the best set of weights to map inputs to outputs in our dataset. We must specify:\n",
    "\n",
    "* The loss function to use to evaluate a set of weights,\n",
    "* The optimizer (used to search through different weights)\n",
    "* Optional metrics we would like to collect and report during training.\n",
    "\n",
    "You know from other topics that loss functions are used to actually identify the parameters in the model. Here are some examples:\n",
    "\n",
    "* Squared error: $\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2$ \n",
    "* Absolute error: $\\sum_{i = 1}^n |y_i - \\hat{y}_i|$\n",
    "* Binary cross-entropy (here, $y$ is binary and $\\hat{y}$ is between 0 and 1): $\\sum_{i = 1}^n -[y\\text{log}(\\hat{y}) - (1-y)\\text{log}(1- \\hat{y})]$\n",
    "* Categorical cross entropy (multi-class classification; $C$ classes): $\\sum_{i = 1}^n \\sum_{c = 1}^C -y_{ic}\\text{log}(\\hat{y}_{ic})$\n",
    "\n",
    "Remember that $\\hat{y}$ is a function of the features and the corresponding parameters (e.g., in logistic regression, a function of $\\beta_0 + \\beta_1x_1 + ... + \\beta_px_p$). You can see that there are different loss functions appropriate for different tasks (classification vs. regression)--since we are classifying a binary variable, we will use binary cross entropy as the loss argument. As optimizer, we use `\"adam\"`: an efficient optimization algorithm. Again, since we are classifying, we will collect and report the classification accuracy, defined via the `metrics` argument (see [the `keras` documentation](https://keras.io/api/metrics/) for more options). Run the line of code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b4214-a9e4-4804-a380-5d359a74734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d9aac-0d2e-4093-adc8-9be69283ebc1",
   "metadata": {},
   "source": [
    "20. Finally, we can fit the model using `.fit()`. Some technical settings/terminology: training occurs over *epochs* and each epoch is split into *batches*.\n",
    "\n",
    "* Epoch: One pass through all of the rows in the training dataset.\n",
    "* Batch: samples considered by the model within an epoch before weights are updated.\n",
    "\n",
    "One epoch is comprised of one or more batches, based on the chosen batch size and the model is fit for many epochs. Run the code chunk below--here, the training data is divided into batches of size 100 - weights are updated after each batch is analyzed.\n",
    "\n",
    "Remember that we are using our training data to fit the model--run the code chunk below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca41211-8b49-4307-941a-a6869671f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_in_pp, Y_in, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de13708-6949-4a42-88ed-8ed4722ce0f2",
   "metadata": {},
   "source": [
    "21. The very last step is evaluation on a training set. We can do so using `.evaluate()`. How does your model perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e357a9-2315-49b0-9df6-a43425eacf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_out_pp, Y_out, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9313c-07a4-4ddc-8026-edc35b74163c",
   "metadata": {},
   "source": [
    "22. When you are done working your way through the neural network steps above, try creating some new neural networks. Be sure to experiment with the number of layers, the units in each layer, the activation functions, the epochs and batches, and the performance metrics. Can you create a new network that outperforms $k$NN?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c6a63-7c08-4fbe-a468-4083a1067a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
