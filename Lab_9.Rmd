---
title: "Lab 9"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(ggplot2)
library(gradethis)
library(learnr)
library(readr)
tutorial_options(exercise.checker = gradethis::grade_learnr)

Boston <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Boston.csv", stringsAsFactors = TRUE)

mtcars <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv", stringsAsFactors = TRUE)
gasprice <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/quantreg/gasprice.csv")

year <- c(1963, 1974, 1981, 1982, 1984, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000)
pairs <- c(417, 791, 1188, 1480, 1757, 1875, 2238, 2475, 2680, 3035, 3399, 3749, 4015, 4449, 4712, 5094, 5295, 5748, 6104, 6471)
eagles <- data.frame(Year = year, Pairs = pairs)

HousePrices <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/AER/HousePrices.csv", stringsAsFactors = TRUE)
```

## Goals

Your goal is to practice using the `lm()` command for linear regression, as well as the associated code.

## Part 1: Loading Packages

Don't forget, packages must be loaded any time you want to use them in an R document. For this lab, we will continue using 3 packages that add extra functionality to R: `readr`, `dplyr`, and `ggplot2`.  Calls to load these packages are in the code chunk below. Go ahead and run this code chunk now.

```{r loadpackages, exercise = T}
library(dplyr)
library(ggplot2)
```

## Part 2: Reading in Data

We will be revisiting the `Boston` dataframe from earlier this semester--it describes housing values in the suburbs of Boston as well as some related variables. You will be working specifically with `medv`, the median value of owner-occupied homes in thousands of dollars for each suburb, and `rm`, average number of rooms per dwelling in each suburb. There are many variables in this dataset--read more at the [dataset documentation](https://vincentarelbundock.github.io/Rdatasets/doc/MASS/Boston.html).  

If you have time, you can also get started on next week's material! There are three more datasets included below (`mtcars`, `gasprice`, `HousePrices`), plus a small dataset that I entered manually (`eagles`).  

Run the code below now to read in the data sets.  No need to modify this code, but remember how to use these functions to read in data in the future!

```{r Boston, exercise = T}
Boston <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Boston.csv", stringsAsFactors = TRUE)

mtcars <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv", stringsAsFactors = TRUE)
gasprice <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/quantreg/gasprice.csv")
HousePrices <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/AER/HousePrices.csv", stringsAsFactors = TRUE)
glimpse(eagles)
```

## Part 3: Data Exploration

Before we start our analysis, let's do some preliminary data exploration. 

### Histograms

For $t$-tests and ANOVA, we used histograms and boxplots to look at the distributions of data. We will still use histograms, especially when considering the distribution of the response variable.

Make a histogram of `medv` with 20 bins. How would you describe this histogram? 

```{r histogram1, exercise = TRUE}

```

```{r histogram1-solution}
ggplot(data = Boston, aes(x = medv)) + 
  geom_histogram(bins = 20)
```

```{r histogram1-check, message = FALSE, warning = FALSE}
grade_code()
```

```{r histogram2, echo=FALSE}
question("What assumption of linear regression does this plot support?",
  answer("Normality", correct = TRUE),
  answer("Linearity"),
  answer("Independence"),
  allow_retry = T,
  random_answer_order = TRUE
)
```

Are there any additional assumptions you might be concerned with?

### Scatterplots

One additional important tool for linear regression is the scatterplot. Using scatterplots, we can visually investigate the relationship between the explanatory and response variables.

We are going to continue learning how to work with `ggplot2`. In `ggplot2`, scatter plots need `x` and `y` variables in the `aes()` statement. The correct geom is `geom_point()`. 

Fill in the `x` and `y` variables in the code below. 

```{r scatter1, exercise = TRUE}
ggplot(data = Boston, aes(x = , y = )) +
  geom_point()
```

```{r scatter1-solution}
ggplot(data = Boston, aes(x = rm, y = medv)) +
  geom_point()
```

```{r scatter1-check, message = FALSE, warning = FALSE}
grade_code()
```

```{r scatter2, echo=FALSE}
question("What assumption of linear regression does this plot support?",
  answer("Linearity", correct = TRUE),
  answer("Normality"),
  answer("Independence"),
  allow_retry = T,
  random_answer_order = TRUE
)
```

Using information you learned in your introductory classes, how might you describe this plot?

```{r scatter3, echo=FALSE}
question("The plot shows an __________ association",
  answer("Positive", correct = TRUE),
  answer("Negative", message = "This graph shows a positive, or increasing, trend--bigger values of the explanatory variable are associated with bigger values of the response variable."),
  allow_retry = T,
  random_answer_order = TRUE
)
```

```{r scatter4, echo=FALSE}
question("The plot shows a __________ association",
  answer("Strong", correct = TRUE),
  answer("Moderate", message = "If we drew a line through the data, most of the points would be close to the line rather than further away."),
  answer("Weak", message = "If we drew a line through the data, most of the points would be close to the line rather than further away."),
  allow_retry = T,
  random_answer_order = TRUE
)
```

In addition to these two questions, do there appear to be any outliers in the dataset?

We can also add the least squares line to the plot using a new geom called `geom_smooth()`. 

```{r scatter5, exercise = TRUE}
ggplot(data = Boston, aes(x = rm, y = medv)) +
  geom_point() + 
  geom_smooth()
```

This curve is definitely not a straight line! That's because `ggplot2` uses a more complex method as the default. You can switch this by using the argument `method = "lm"` inside `geom_smooth()` (`"lm"` for **L**inear **M**odel). 

```{r scatter6, exercise = TRUE}
ggplot(data = Boston, aes(x = rm, y = medv)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

Much better! Now, take a look at the gray bar around the blue line. This is actually the prediction error of the individual responses, $\hat{Y}$. It is useful to have, but in case you ever want to remove it, you can add the argument `se = FALSE` (`se` for **S**tandard **E**rror).

```{r scatter7, exercise = TRUE}
ggplot(data = Boston, aes(x = rm, y = medv)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

### Summary Statistics

Now let's report some summary statistics for the explanatory variable. In addition to the mean and standard deviations, we might be interested in the minimum and maximum values of each variable since it will let us know what values are "safe" to use for predictions. Using the `dplyr` package and pipe operator (`%>%`), find the sample mean and standard deviation, as well as the minimum and maximum values.

```{r sumstats, exercise = TRUE}
Boston %>%
  summarise(Mean = , SD = , n = , Min = , Max = )
```

```{r sumstats-solution}
Boston %>%
  summarise(Mean = mean(rm), SD = sd(rm), n = n(), Min = min(rm), Max = max(rm))
```

```{r sumstats-check, message = FALSE, warning = FALSE}
grade_code()
```

```{r sumstats2, echo=FALSE}
question("For which value of room would it be inappropriate to predict median value?",
  answer("3", correct = TRUE),
  answer("4", message = "This value is greater than the minimum value of the room variable."),
  answer("5",message = "This value is greater than the minimum value of the room variable."),
  answer("6",message = "This value is greater than the minimum value of the room variable."),
  allow_retry = T,
  random_answer_order = TRUE
)
```

## Part 4: Model Fitting

Fit a model, named `m_rm`, that predicts the median value from the average number of rooms. 

```{r prepare-model1, message = FALSE}
m_rm <- lm(data = Boston, formula = medv ~ rm)

#b1 <- summary(m_rm)$coefficients[2,1]
#se_b1 <- summary(m_rm)$coefficients[2,2]
```

```{r model1, exercise = TRUE, exercise.setup = "prepare-model1"}
m_rm <-
```

```{r model1-solution}
m_rm <- lm(data = Boston, formula = medv ~ rm)
```

```{r model1-check, message = FALSE, warning = FALSE}
grade_code()
```

## Part 5: Residual Plots

We have not discussed residual plots yet, but they are very useful for linear regression as well as for ANOVA--they similarly can help us investigate the assumptions of independence and outlying points. Let's go piece by piece. 

First, we need to "calculate" the $\hat{Y}_i$ values--luckily, `lm()` is a super powerful function and automatically predicts them for us! Can you extract them from `m_rm`? 
  
```{r residualplot1, exercise = TRUE, exercise.setup = "prepare-model1"}

```

```{r residualplot1-solution}
m_rm$fitted.values
```

```{r residualplot1-check, message = FALSE, warning = FALSE}
grade_code()
```

<div id="residualplot1-hint">
**Hint:** Use the `attributes()` function if you can't remember what the $\hat{Y}_i$ are called!
</div>

We also need to extract the residuals from `m_rm`. 

```{r residualplot2, exercise = TRUE, exercise.setup = "prepare-model1"}

```

```{r residualplot2-solution}
m_rm$residuals
```

```{r residualplot2-check, message = FALSE, warning = FALSE}
grade_code()
```

<!-- Now, we have extracted the fitted values and residuals, but we need them to be stored together. We can do this using `cbind()`, which stands for Column BIND. Bind these columns together, and save them as a new object `resplot`.  -->

<!-- ```{r residualplot3, exercise = TRUE} -->
<!-- resplot <-  -->
<!-- ``` -->

<!-- ```{r residualplot3-solution} -->
<!-- resplot <- cbind(m_rm$fitted.values, m_rm$residuals) -->
<!-- ``` -->

<!-- ```{r residualplot3-check, message = FALSE, warning = FALSE} -->
<!-- grade_code() -->
<!-- ``` -->

Using these values, we can create a residual plot. HOWEVER--we can create a plot directly from the model we fit earlier, just like we did in ANOVA! As with ANOVA, the fitted values go on the $x$-axis, and the residuals go on the $y$-axis. You can use the `geom_point()` function to add the points to the plot.

```{r residualplot5, exercise = TRUE, exercise.setup = "prepare-model1"}

```

```{r residualplot5-solution}
ggplot(data = m_rm, aes(x = .fitted, y = .resid)) + 
  geom_point()
```

```{r residualplot5-check, message = FALSE, warning = FALSE}
grade_code()
```

Remember that we are looking for the lack of a pattern in the residual plot. One way to help find trends, increasing, decreasing, or otherwise, is to add a red line through $y = 0$. To do this, you can use the `geom_hline()` command. 

Use the help menu to try and add a red line through $y = 0$ to your plot. 

```{r residualplot6, exercise = TRUE, exercise.setup = "prepare-model1"}
ggplot(data = m_rm, aes(x = .fitted, y = .resid)) + 
  geom_point() + 
  geom_hline()
```

```{r residualplot6-solution}
ggplot(data = m_rm, aes(x = .fitted, y = .resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0, color = "red")
```

```{r residualplot6-check, message = FALSE, warning = FALSE}
grade_code()
```

Are there any trends or points that concern you?

## Part 6: Investigating Slopes and Intercepts

We have already fit the linear regression model. What are the values of $b_0$ and $b_1$? 

```{r equation, exercise = TRUE, exercise.setup = "prepare-model1"}

```

```{r equation-solution}
m_rm
```

```{r equation-check, message = FALSE, warning = FALSE}
grade_code()
```

### Linear Regression Equation

Write the equation of the line after you find these values. 

### Interpreting Slope

How would you interpret the slope? 

### Interpreting Intercept

How would you interpret the intercept? Does it make sense to interpret the intercept? Why or why not?

### Tests on the Slope

Run a test to see if $\beta_1 = 0$. Is there evidence of a difference?

```{r slopetest, exercise = TRUE, exercise.setup = "prepare-model1"}

```

```{r slopetest-solution}
summary(m_rm)
```

```{r slopetest-check, message = FALSE, warning = FALSE}
grade_code()
```

<div id="slopetest-hint">
**Hint:** Remember that the default output of `summary()` returns a hypothesis test for $\beta_0$ and $\beta_1$ individually being equal to 0. 
</div>

Now, let's run a test to see if an additional room results in an increase in the median value of $\$10,000$. Remember that you have to "manually" do this. First, we need to calculate the $t$-score using the equation

$$t = \frac{b_1 - \beta_1}{SE(b_1)}$$

This means we need the values of $b_1$ and $SE(b_1)$. You could manually type them in, or you can extract them from `summary(m_rm)` using a combination of the `$` and the `[]`. The square brackets in R refer to different locations within an object--you can read more about them at [http://adv-r.had.co.nz/Subsetting.html#subsetting-operators](this Advanced R tutorial).

We have been using the `$` with a lot of data frames, but you can also use them lists. Remember that unlike a vector or a dataframe, lists can have different types of elements. We can find the names of these elements using `attributes()`. Run the code below to see everything contained in `summary(m_rm)`. 

```{r slopetest2, exercise = TRUE, exercise.setup = "prepare-model1"}
attributes(summary(m_rm))
```

The values we want are stored as `coefficients` (even though it's called `coefficients`, it also contains the standard errors). Print it out by running the code below. 

```{r slopetest3, exercise = TRUE, exercise.setup = "prepare-model1"}
summary(m_rm)$coefficients
```

Now, we can extract the values and save them. First, let's extract the value of $b_1$. It is in the second row, first column. If we put those locations inside the square brackets, as below, we can pull out $b_1$ and print it out to confirm it's the correct value we want. 

```{r slopetest4, exercise = TRUE, exercise.setup = "prepare-model1"}
b1 <- summary(m_rm)$coefficients[2,1]
b1
```

Now, try changing the location to get $SE(b_1)$.

```{r slopetest5, exercise = TRUE, exercise.setup = "prepare-model1"}
se_b1 <- 
```

```{r slopetest5-solution}
se_b1 <- summary(m_rm)$coefficients[2,2]
se_b1
```

```{r slopetest5-check, message = FALSE, warning = FALSE}
grade_code()
```

<div id="slopetest5-hint">
**Hint:** Remember that $SE(b_1)$ is in the second row, second column.
</div>

Now, using $\beta_1 = 10$ (remember that the measurements are recorded in thousands of dollars), compute the $t$-statistic. 

```{r prepare-model2, message = FALSE}
m_rm <- lm(data = Boston, formula = medv ~ rm)

b1 <- summary(m_rm)$coefficients[2,1]
se_b1 <- summary(m_rm)$coefficients[2,2]
```

```{r slopetest6, exercise = TRUE, exercise.setup = "prepare-model2"}
t_statistic <- 
```

```{r slopetest6-solution}
t_statistic <- (b1 - 10)/se_b1
```

```{r slopetest6-check, message = FALSE, warning = FALSE}
grade_code()
```

<div id="slopetest-hint6">
**Hint:** Remember that $SE(b_1)$ is in the second row, second column.
</div>

Finally, we can calculate the p-value using the following code. Would you reject or fail to reject the null hypothesis that $\beta_1 = 10$?

```{r prepare-model3, message = FALSE}
m_rm <- lm(data = Boston, formula = medv ~ rm)

b1 <- summary(m_rm)$coefficients[2,1]
se_b1 <- summary(m_rm)$coefficients[2,2]

t_statistic <- (b1 - 10)/se_b1
```

```{r slopetest7, exercise = TRUE, exercise.setup = "prepare-model3"}
 2*pt(abs(t_statistic), df = 504, lower.tail = F) 
```

## Part 7: Confidence and Prediction Intervals

Finally, we can practice creating confidence and prediction intervals. 

Using the function `predict.lm()`, create a confidence interval for the average median value in a neighborhood with 5.5 rooms. 

```{r confint1, exercise = TRUE, exercise.setup = "prepare-model3"}
predict.lm()
```

```{r confint1-solution}
predict.lm(m_rm, newdata=data.frame(rm=5.5),
           interval="confidence", level=0.95)
```

```{r confint1-check, message = FALSE, warning = FALSE}
grade_code()
```

<div id="confint-hint">
**Hint:** Make sure in the `data.frame()` command, you are including the name of the variable you are using for the predictions (`rm`).
</div>

Now, create confidence intervals for the average median value in neighborhoods with 4, 5.5, 7, and 8.5 rooms. Write this as one line of code, and make sure to specify the Bonferroni-corrected confidence levels!

```{r confint2, exercise = TRUE, exercise.setup = "prepare-model3"}
predict.lm()
```

```{r confint2-solution}
predict.lm(m_rm, newdata=data.frame(rm=c(4, 5.5, 7, 8.5)),
           interval="confidence", level=(1-0.05/4))
```

```{r confint2-check, message = FALSE, warning = FALSE}
grade_code()
```

Would you expect the confidence interval around 5.5 to be narrower or wider than the confidence interval around 8? Why?

Now, create a prediction interval around 5.5. 

```{r predint1, exercise = TRUE, exercise.setup = "prepare-model3"}
predict.lm()
```

```{r predint1-solution}
predict.lm(m_rm, newdata=data.frame(rm=c(5.5)),
           interval="prediction", level=0.95)
```

```{r predint1-check, message = FALSE, warning = FALSE}
grade_code()
```

Would you expect the prediction interval around 5.5 to be narrower or wider than the confidence interval around 5.5? Why?

## Part 8: More Trends and Residual Plots

If you are done, you can get a head start on next week's material. Use the datasets below to create scatterplots, and discuss with your group members whether you think a relationship is linear or not. If not linear, what would you try instead?

### `mtcars`

The `mtcars` dataset includes data on fuel consumption and other aspects of 32 cars from a 1974 issue of *Motor Trends* magazine [documentation here](https://vincentarelbundock.github.io/Rdatasets/doc/datasets/mtcars.html). We are specifically interested in how the weight of the car (`wt`) might impact the car's displacement, which can be thought of as the size of the engine (`disp`).

Create a scatterplot displaying the relationship between weight and displacement. Notice that unlike the other plots we have created, such as histograms and densities, you have to supply an `x` and a `y` aesthetic. Remember that $x$ is traditionally the explanatory variable and $y$ is traditionally the response.

```{r mtcars, exercise = TRUE}
ggplot(data = mtcars, aes(x = , y = )) +
  geom_point()
```

```{r mtcars-solution, message = FALSE, warning = FALSE, echo = FALSE, echo = FALSE}
ggplot(data = mtcars, aes(x = wt, y = disp)) +
  geom_point()
```

```{r mtcars-check, message = FALSE, warning = FALSE}
grade_code()
```

Describe this relationship in terms of its form, direction, strength, and unusual values. Is a linear relationship appropriate for describing weight and displacement? If not, what type of relationship might be more appropriate?

```{r mtcars2, echo=FALSE}
question("Which of the following describe the relationship between weight and engine displacement? Note that the correlation between the variables is 0.888. Select all that apply.",
  answer("Linear", correct = TRUE),
  answer("Non-Linear"),
  answer("Negative"),
  answer("Positive", correct = TRUE),
  answer("Weak"),
  answer("Moderate"),
  answer("Strong", correct = TRUE),
  answer("Unusual Values"),
  answer("No Unusual Values", correct = TRUE),
  allow_retry = T
)
```

Now, create a linear model predicting displacement from weight and plot the residuals. What type of pattern do you see? Does this make it easier to guess what the form of this relationship is?

```{r mtcars3, exercise = TRUE}
m_mtcars <- lm(data = mtcars, )
ggplot(data = m_mtcars, aes(x = , y = )) +
  geom_point()
```

```{r mtcars3-solution, message = FALSE, warning = FALSE, echo = FALSE, echo = FALSE}
m_mtcars <- lm(data = mtcars, disp ~ wt)
ggplot(data = m_mtcars, aes(x = .fitted, y = .resid)) +
  geom_point()
```

```{r mtcars3-check, message = FALSE, warning = FALSE}
grade_code()
```

### `gasprice`

The `gasprice` dataset includes data on gas prices in the United States, collected once per week from 1990 to 2003 [documentation here](https://vincentarelbundock.github.io/Rdatasets/doc/quantreg/gasprice.html).

Create a scatterplot displaying the relationship between time (`time`) and gas price (`value`).

```{r gasprice, exercise = TRUE}
ggplot(data = , aes()) +
  geom_point()
```

```{r gasprice-solution, message = FALSE, warning = FALSE, echo = FALSE, echo = FALSE}
ggplot(gasprice, aes(x = time, y = value)) +
  geom_point()
```

```{r gasprice-check, message = FALSE, warning = FALSE}
grade_code()
```

```{r gasprice2, echo=FALSE}
question("Which of the following describe the relationship between gas prices and time? Note that the correlation between the variables is 0.664. Select all that apply.",
  answer("Linear"),
  answer("Non-Linear", correct = TRUE),
  answer("Negative"),
  answer("Positive", correct = TRUE),
  answer("Weak"),
  answer("Moderate"),
  answer("Strong", correct = TRUE),
  answer("Unusual Values"),
  answer("No Unusual Values", correct = TRUE),
  allow_retry = T
)
```

Now, create a linear model predicting gas price from time and plot the residuals. What type of pattern do you see? Does this make it easier to guess what the form of this relationship is?

```{r gasprice3, exercise = TRUE}
m_gasprice <- lm( , )
ggplot( , aes(x = , y = )) +
  geom_point()
```

```{r gasprice3-solution, message = FALSE, warning = FALSE, echo = FALSE, echo = FALSE}
m_gasprice <- lm(data = gasprice, value ~ time)
ggplot(data = m_gasprice, aes(x = .fitted, y = .resid)) +
  geom_point()
```

```{r gasprice3-check, message = FALSE, warning = FALSE}
grade_code()
```

Not particularly. This is actually an example of a time series! If your graph looks like this, do not try to use linear regression.

### `eagles`

The `eagles` dataset contains information the number of mating pairs of bald eagles in the United States. In 1967, bald eagles officially became an endangered species, and in 1972, the use of DDT (a pesticide causing damage to the shells of the eagle eggs) was banned--scientists have been tracking the population since (<https://courses.lumenlearning.com/wmopen-concepts-statistics/chapter/exponential-relationships-1-of-6/>).
Create a scatterplot displaying the relationship between time (`Year`) and the number of mating pairs (`Pairs`).

```{r eagles1, exercise = TRUE}
ggplot() +
  geom_point()
```

```{r eagles1-solution, message = FALSE, warning = FALSE, echo = FALSE, echo = FALSE}
ggplot(eagles, aes(x = Year, y = Pairs)) +
  geom_point()
```

```{r eagles1-check, message = FALSE, warning = FALSE}
grade_code()
```

```{r eagles2, echo=FALSE}
question("Which of the following describe the relationship between mating pairs of eagles and year? Note that the correlation between the variables is 0.914. Select all that apply.",
  answer("Linear", correct = TRUE),
  answer("Non-Linear"),
  answer("Negative"),
  answer("Positive", correct = TRUE),
  answer("Weak"),
  answer("Moderate"),
  answer("Strong", correct = TRUE),
  answer("Unusual Values", correct = TRUE),
  answer("No Unusual Values"),
  allow_retry = T
)
```

Now, create a linear model predicting pairs from year and plot the residuals. What type of pattern do you see? Does this make it easier to guess what the form of this relationship is?

```{r eagles3, exercise = TRUE}
m_eagles <- 
ggplot() +
  geom_point()
```

```{r eagles3-solution, message = FALSE, warning = FALSE, echo = FALSE, echo = FALSE}
m_eagles <- lm(data = eagles, Pairs ~ Year)
ggplot(data = m_eagles, aes(x = .fitted, y = .resid)) +
  geom_point()
```

```{r eagles3-check, message = FALSE, warning = FALSE}
grade_code()
```

### `HousePrices`

The `HousePrices` dataset is another real estate dataset, containing information on sales prices of houses sold in Windsor, Canada, during July, August, and September of 1987 [documentation here](https://vincentarelbundock.github.io/Rdatasets/doc/AER/HousePrices.html). The dataset also includes information on many other characteristics, including `aircon` (whether the house had air conditioning) and `driveway` (whether the house had a driveway). Don't forget, between the `glimpse()`, `colnames()`, and `?` function, you should be able to read about each variable!

Create a scatterplot displaying the relationship between lot size, in square feet (`lotsize`) and the price of the house (`price`).

```{r windsor1, exercise = TRUE}
ggplot()

```

```{r windsor1-solution, message = FALSE, warning = FALSE, echo = FALSE, echo = FALSE}
ggplot(HousePrices, aes(x = lotsize, y = price)) +
  geom_point()
```

```{r windsor1-check, message = FALSE, warning = FALSE}
grade_code()
```

```{r windsor2, echo=FALSE}
question("Which of the following describe the relationship between mating pairs of lot size and price? Note that the correlation between the variables is 0.536. Select all that apply.",
  answer("Linear", correct = TRUE),
  answer("Non-Linear"),
  answer("Negative"),
  answer("Positive", correct = TRUE),
  answer("Weak"),
  answer("Moderate", correct = TRUE),
  answer("Strong"),
  answer("Unusual Values"),
  answer("No Unusual Values", correct = TRUE),
  allow_retry = T,
  random_answer_order = TRUE
)
```

Now, create a linear model predicting price from lot size and plot the residuals. What type of pattern do you see? Does this make it easier to guess what the form of this relationship is?

```{r windsor3, exercise = TRUE}
m_windsor <- 

```

```{r windsor3-solution, message = FALSE, warning = FALSE, echo = FALSE, echo = FALSE}
m_windsor <- lm(data = HousePrices, price ~ lotsize)
ggplot(data = m_windsor, aes(x = .fitted, y = .resid)) +
  geom_point()
```

```{r windsor3-check, message = FALSE, warning = FALSE}
grade_code()
```
